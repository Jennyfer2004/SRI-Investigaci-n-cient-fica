{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "df = pd.read_csv(\"/home/jennifer/Documentos/tercer_año/segundo_semestre/SRI/proyecto final/papers_guardados.csv\")  \n",
    "a=0\n",
    "def remove_sections(text,urls):\n",
    "    global a\n",
    "    a+=1\n",
    "    after_refs = re.split(r'\\n(references|bibliography|acknowledg(e)?ments)\\b', text, flags=re.IGNORECASE)  \n",
    "\n",
    "    url_patron= r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "    for i in after_refs:\n",
    "        url=[]\n",
    "        if not i:\n",
    "            continue\n",
    "        if len(i)==0:\n",
    "            continue\n",
    "        url = re.findall(url_patron, i)\n",
    "        url1 = [re.sub(r'[).,;]+$', '', url_sin_parentesis.strip()) for url_sin_parentesis in url if url_sin_parentesis.strip()]\n",
    "        if url1:\n",
    "            for k in url1:\n",
    "                urls.append(k)\n",
    "    print(f\"Total de papers visitados {a}\")\n",
    "    return after_refs[0]\n",
    "\n",
    "urls_collected = []\n",
    "df['clean_contenido'] = df['contenido'].apply(lambda x: remove_sections(x, urls_collected))\n",
    "\n",
    "# Guardar las URLs en un archivo\n",
    "urls_df = pd.DataFrame({'URL': urls_collected})\n",
    "urls_df.to_csv('urls_encontradas.csv', index=False)  # Guardar como CSV\n",
    "# Alternativa para guardar como TXT:\n",
    "# with open('urls_encontradas.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(urls_collected))\n",
    "\n",
    "print(f\"Se encontraron y guardaron {len(urls_collected)} URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93631db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "import re\n",
    "import os \n",
    "\n",
    "VISITED_DOIS_FILE = \"dois_visitados.txt\"\n",
    "OUTPUT_FILE = \"../proyecto final/papers_guardados.csv\"\n",
    "\n",
    "if os.path.exists(VISITED_DOIS_FILE):\n",
    "    with open(VISITED_DOIS_FILE, \"r\") as f:\n",
    "        existing_dois = set(f.read().splitlines())\n",
    "else:\n",
    "    existing_dois = set()\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existente = pd.read_csv(OUTPUT_FILE)\n",
    "    dois_guardados = set(df_existente[\"doi\"])\n",
    "    results = df_existente.to_dict(orient=\"records\")\n",
    "else:\n",
    "    df_existente = pd.DataFrame()\n",
    "    dois_guardados = set()\n",
    "    results = []\n",
    "print(results)\n",
    "# EMAIL = \"jennifersanchezsantana36@gemail.com\"\n",
    "\n",
    "def obtener_datos_unpaywall(doi, email):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\" \n",
    "    params = {\n",
    "        \"email\": email  # Es OBLIGATORIO incluir tu email\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"❌ Error de conexión para DOI {doi}: {str(e)}\")\n",
    "        time.sleep(5)  # Espera más ante errores de conexión\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⚠️ Error HTTP para DOI {doi}: {str(e)}\")\n",
    "        return None\n",
    "def extract_text_from_pdf(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        time.sleep(1)\n",
    "\n",
    "        if \"application/pdf\" not in response.headers.get(\"Content-Type\", \"\"):\n",
    "            return None\n",
    "        with BytesIO(response.content) as f:\n",
    "            reader = PdfReader(f)\n",
    "            return \"\\n\".join(filter(None, (p.extract_text() for p in reader.pages))).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error leyendo PDF: {e}\")\n",
    "        return None\n",
    "    \n",
    "def extraer_info_relevante(datos):\n",
    "    title = datos.get(\"title\", \"\")\n",
    "    doi = datos.get(\"doi\", \"\")\n",
    "    if not datos:\n",
    "        return None\n",
    "    if not datos.get(\"best_oa_location\"):\n",
    "        return None \n",
    "    landing_page = datos.get(\"best_oa_location\", {}).get(\"url_for_landing_page\", \"\")\n",
    "    \n",
    "    # Unir todos los nombres de autores\n",
    "    authors = datos.get(\"z_authors\", [])\n",
    "    authors_str = \", \".join([a.get(\"raw_author_name\", \"\") for a in authors])\n",
    "    \n",
    "    published = datos.get(\"published_date\", \"\")\n",
    "    language = datos.get(\"language\", \"en\")\n",
    "    \n",
    "    # No siempre está presente\n",
    "    abstract = datos.get(\"abstract\", \"No abstract available.\")\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"doi\": doi,\n",
    "        \"landing_page\": landing_page,\n",
    "        \"authors\": authors_str,\n",
    "        \"published\": published,\n",
    "        \"abstract\": abstract,\n",
    "        \"language\": language\n",
    "    }\n",
    "def find_direct_pdf_link(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if \"text/html\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                if \".pdf\" in link[\"href\"].lower():\n",
    "                    return requests.compat.urljoin(url, link[\"href\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PDF manual falló: {e}\")\n",
    "    return None\n",
    "def get_open_access_pdf(doi, email=EMAIL):\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\"\n",
    "    params = {\"email\": email}\n",
    "    try:\n",
    "        response = requests.get(url, params=params).json()\n",
    "        if response.get(\"is_oa\"):\n",
    "            return response.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en Unpaywall: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_dois_from_text(text):\n",
    "    # DOI estándar: 10.<registro>/<sufijo>\n",
    "    doi_pattern = r\"10.\\d{4,9}/[-._;()/:A-Z0-9]+\"\n",
    "    return re.findall(doi_pattern, text, flags=re.IGNORECASE)\n",
    "def is_doi(url):\n",
    "    \"\"\"Verifica si la URL es un DOI.\"\"\"\n",
    "    return \"doi.org/10\" in url.lower()\n",
    "a=['https://doi.org/10.12725/ujbm.52.6', 'https://doi.org/10.3233/IDT', 'https://doi.org/10.4236/ojbm.2018.63048', 'https://doi.org/10.21511/im.15(3).2019.04', 'https://doi.org/10.1016/j.ijtst.2019.09.004', 'https://doi.org/10.1016/bs.adcom.2019.10.006', 'https://doi.org/10.1007/s12652', 'https://doi.org/10.1016/j.eswa.2018.08', 'https://doi.org/10.1109/LRA.2018.2800119']\n",
    "\n",
    "# for url in a:\n",
    "#     if is_doi:\n",
    "#         doi=extract_dois_from_text(url)[0]\n",
    "#         datos = obtener_datos_unpaywall(doi, EMAIL)\n",
    "        \n",
    "#         if datos:\n",
    "#             info = extraer_info_relevante(datos)\n",
    "#             if not info:\n",
    "#                 print(\"❌ Error en url extraida\")\n",
    "#                 continue\n",
    "#             url_pdf = datos.get(\"best_oa_location\", {}).get(\"landing_page\", \"\")\n",
    "#             if not url_pdf:\n",
    "#                 url_pdf=datos.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "#                 if not url_pdf:\n",
    "#                     print(\"⚠️ Url no validada\")\n",
    "#             content = extract_text_from_pdf(url_pdf) if url_pdf else None\n",
    "#             print(content)\n",
    "#             time.sleep(1) \n",
    "count=0\n",
    "for url in urls_collected:\n",
    "    count+=1\n",
    "    print(count)\n",
    "    if is_doi(url):\n",
    "        with open(VISITED_DOIS_FILE, \"a\") as f_dois:\n",
    "            doi=extract_dois_from_text(url)[0]\n",
    "            datos = obtener_datos_unpaywall(doi, EMAIL)\n",
    "            if datos:\n",
    "                if doi in  dois_guardados:\n",
    "                    continue\n",
    "                paper = extraer_info_relevante(datos)\n",
    "                print(\"❌ Error en url extraida\")\n",
    "\n",
    "                if not paper:\n",
    "                    continue\n",
    "                doi = paper[\"doi\"]\n",
    "                existing_dois.add(doi)\n",
    "                landing_url = paper[\"landing_page\"]\n",
    "                pdf_url = get_open_access_pdf(doi)\n",
    "                if not pdf_url:\n",
    "                    pdf_url = find_direct_pdf_link(landing_url)\n",
    "                \n",
    "                paper[\"pdf_url\"] = pdf_url\n",
    "                content = extract_text_from_pdf(pdf_url) if pdf_url else None\n",
    "                paper[\"contenido\"] = content\n",
    "                if content and content!=\"No disponible\":\n",
    "                    results.add(paper)\n",
    "                    dois_guardados.append(doi)\n",
    "                    f_dois.write(doi + \"\\n\")\n",
    "                    total_collected += 1\n",
    "\n",
    "                    print(f\"✅ ({total_collected}) {paper['title'][:60]}...\")\n",
    "                    \n",
    "                    pd.DataFrame([paper]).to_csv(OUTPUT_FILE, mode=\"a\", header=not os.path.exists(OUTPUT_FILE), index=False, encoding='utf-8',errors='ignore')\n",
    "                else:\n",
    "                    print(f\"⚠️ Sin contenido: {paper['title'][:60]}\")\n",
    "\n",
    "                time.sleep(1)  \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
