{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c479b2",
   "metadata": {},
   "source": [
    "Extraer urls referenciadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"../segundo_semestre/SRI/proyecto final/papers_guardados.csv\")  \n",
    "a=0\n",
    "def remove_sections(text,urls):\n",
    "    \"\"\"\n",
    "    Remover sección de referencias y añadir las url a la lista \n",
    "    Args:\n",
    "        text(str): texto del artićulo\n",
    "        urls(list): lista global de referencias encontradas\n",
    "    Returns:\n",
    "        str: sin referencias\n",
    "    \"\"\"\n",
    "    global a\n",
    "    a+=1\n",
    "    after_refs = re.split(r'\\n(references|bibliography|acknowledg(e)?ments)\\b', text, flags=re.IGNORECASE)  \n",
    "\n",
    "    url_patron= r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "    for i in after_refs:\n",
    "        url=[]\n",
    "        if not i:\n",
    "            continue\n",
    "        if len(i)==0:\n",
    "            continue\n",
    "        url = re.findall(url_patron, i)\n",
    "        url1 = [re.sub(r'[).,;]+$', '', url_sin_parentesis.strip()) for url_sin_parentesis in url if url_sin_parentesis.strip()]\n",
    "        if url1:\n",
    "            for k in url1:\n",
    "                urls.append(k)\n",
    "    print(f\"Total de papers visitados {a}\")\n",
    "    return after_refs[0]\n",
    "\n",
    "urls_collected = []\n",
    "df['clean_contenido'] = df['contenido'].apply(lambda x: remove_sections(x, urls_collected))\n",
    "\n",
    "urls_df = pd.DataFrame({'URL': urls_collected})\n",
    "urls_df.to_csv('urls_encontradas.csv', index=False) \n",
    "\n",
    "print(f\"Se encontraron y guardaron {len(urls_collected)} URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93631db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "EMAIL=os.getenv(\"email\")\n",
    "\n",
    "VISITED_DOIS_FILE = \"dois_visitados.txt\"\n",
    "OUTPUT_FILE = \"../proyecto final/papers_guardados.csv\"\n",
    "\n",
    "if os.path.exists(VISITED_DOIS_FILE):\n",
    "    with open(VISITED_DOIS_FILE, \"r\") as f:\n",
    "        existing_dois = set(f.read().splitlines())\n",
    "else:\n",
    "    existing_dois = set()\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existente = pd.read_csv(OUTPUT_FILE)\n",
    "    dois_guardados = set(df_existente[\"doi\"])\n",
    "    results = df_existente.to_dict(orient=\"records\")\n",
    "else:\n",
    "    df_existente = pd.DataFrame()\n",
    "    dois_guardados = set()\n",
    "    results = []\n",
    "print(results)\n",
    "\n",
    "def obtener_datos_unpaywall(doi, email):\n",
    "    \"\"\"\n",
    "    Obtiene información sobre la disponibilidad de doi usando la api de unpaywall.\n",
    "    Args:\n",
    "        doi(str): el identificador de un artículo .\n",
    "        email(str): correo electrónico registrado para usar la API de Unpaywall.\n",
    "    Returns:\n",
    "        dict or None: json de la API si es exitosa, None en caso de error.\n",
    "   \n",
    "   \"\"\"\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\" \n",
    "    params = {\n",
    "        \"email\": email \n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(f\"❌ Error de conexión para DOI {doi}: {str(e)}\")\n",
    "        time.sleep(5) \n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"⚠️ Error HTTP para DOI {doi}: {str(e)}\")\n",
    "        return None\n",
    "def extract_text_from_pdf(pdf_url):\n",
    "    \"\"\"\n",
    "    Extrae el texto de un archivo PDF desde una url\n",
    "    Args:\n",
    "        pdf_url(str): url del pdf\n",
    "    Returns:\n",
    "        str: Texto extraído del PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        time.sleep(1)\n",
    "\n",
    "        if \"application/pdf\" not in response.headers.get(\"Content-Type\", \"\"):\n",
    "            return None\n",
    "        with BytesIO(response.content) as f:\n",
    "            reader = PdfReader(f)\n",
    "            return \"\\n\".join(filter(None, (p.extract_text() for p in reader.pages))).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error leyendo PDF: {e}\")\n",
    "        return None\n",
    "    \n",
    "def extraer_info_relevante(datos):\n",
    "    \"\"\"\n",
    "    Extrae información sobre tos artículos\n",
    "    Args:\n",
    "        datos (dict): datos obtenidos desde la api de unpaywall.\n",
    "    Returns:\n",
    "        dict : Diccionario con los datos relevantes\n",
    "    \"\"\"\n",
    "    title = datos.get(\"title\", \"\")\n",
    "    doi = datos.get(\"doi\", \"\")\n",
    "    if not datos:\n",
    "        return None\n",
    "    if not datos.get(\"best_oa_location\"):\n",
    "        return None \n",
    "    landing_page = datos.get(\"best_oa_location\", {}).get(\"url_for_landing_page\", \"\")\n",
    "    \n",
    "    authors = datos.get(\"z_authors\", [])\n",
    "    authors_str = \", \".join([a.get(\"raw_author_name\", \"\") for a in authors])\n",
    "    \n",
    "    published = datos.get(\"published_date\", \"\")\n",
    "    language = datos.get(\"language\", \"en\")\n",
    "    \n",
    "    abstract = datos.get(\"abstract\", \"No abstract available.\")\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"doi\": doi,\n",
    "        \"landing_page\": landing_page,\n",
    "        \"authors\": authors_str,\n",
    "        \"published\": published,\n",
    "        \"abstract\": abstract,\n",
    "        \"language\": language\n",
    "    }\n",
    "def find_direct_pdf_link(url):\n",
    "    \"\"\"\n",
    "    Intenta encontrar un enlace directo a PDF en una página web\n",
    "    Args:\n",
    "        url(str): url de la página donde buscar el enlace del pdf\n",
    "    Returns:\n",
    "        str: url completa del PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if \"text/html\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                if \".pdf\" in link[\"href\"].lower():\n",
    "                    return requests.compat.urljoin(url, link[\"href\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PDF manual falló: {e}\")\n",
    "    return None\n",
    "def get_open_access_pdf(doi, email=EMAIL):\n",
    "    \"\"\"\n",
    "    Busca con la api unpaywall el contenidos de los artículos\n",
    "    Args:\n",
    "       doi(str): identificador del artículo\n",
    "       email(str): correo electrónico registrado para usar la API de Unpaywall.\n",
    "    Returns:\n",
    "       str: url del pdf\n",
    "    \"\"\"\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\"\n",
    "    params = {\"email\": email}\n",
    "    try:\n",
    "        response = requests.get(url, params=params).json()\n",
    "        if response.get(\"is_oa\"):\n",
    "            return response.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en Unpaywall: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_dois_from_text(text):\n",
    "    \"\"\"\n",
    "    Extrae los dois encontrados dentro de un texto dado.\n",
    "    Args:\n",
    "        text (str): Texto donde se realizará la búsqueda de dois.\n",
    "    Returns:\n",
    "        list: Lista de dois encontrados en el texto.\n",
    "    \"\"\"\n",
    "    # DOI estándar: 10.<registro>/<sufijo>\n",
    "    doi_pattern = r\"10.\\d{4,9}/[-._;()/:A-Z0-9]+\"\n",
    "    return re.findall(doi_pattern, text, flags=re.IGNORECASE)\n",
    "\n",
    "def is_doi(url):\n",
    "    \"\"\"\n",
    "    Verifica si la url es un doi o lo contiene.\n",
    "    Args:\n",
    "        url (str): url que se va a comprobar.\n",
    "    Returns:\n",
    "        bool: True si la url contiene un doi, False en caso contrario.\n",
    "        \"\"\"\n",
    "    return \"doi.org/10\" in url.lower()\n",
    "\n",
    "count=0\n",
    "for url in urls_collected:\n",
    "    count+=1\n",
    "    print(count)\n",
    "    if is_doi(url):\n",
    "        with open(VISITED_DOIS_FILE, \"a\") as f_dois:\n",
    "            doi=extract_dois_from_text(url)[0]\n",
    "            datos = obtener_datos_unpaywall(doi, EMAIL)\n",
    "            if datos:\n",
    "                if doi in  dois_guardados:\n",
    "                    continue\n",
    "                paper = extraer_info_relevante(datos)\n",
    "                print(\"❌ Error en url extraida\")\n",
    "\n",
    "                if not paper:\n",
    "                    continue\n",
    "                doi = paper[\"doi\"]\n",
    "                existing_dois.add(doi)\n",
    "                landing_url = paper[\"landing_page\"]\n",
    "                pdf_url = get_open_access_pdf(doi)\n",
    "                if not pdf_url:\n",
    "                    pdf_url = find_direct_pdf_link(landing_url)\n",
    "                \n",
    "                paper[\"pdf_url\"] = pdf_url\n",
    "                content = extract_text_from_pdf(pdf_url) if pdf_url else None\n",
    "                paper[\"contenido\"] = content\n",
    "                if content and content!=\"No disponible\":\n",
    "                    results.add(paper)\n",
    "                    dois_guardados.append(doi)\n",
    "                    f_dois.write(doi + \"\\n\")\n",
    "                    total_collected += 1\n",
    "\n",
    "                    print(f\"✅ ({total_collected}) {paper['title'][:60]}...\")\n",
    "                    \n",
    "                    pd.DataFrame([paper]).to_csv(OUTPUT_FILE, mode=\"a\", header=not os.path.exists(OUTPUT_FILE), index=False, encoding='utf-8',errors='ignore')\n",
    "                else:\n",
    "                    print(f\"⚠️ Sin contenido: {paper['title'][:60]}\")\n",
    "\n",
    "                time.sleep(1)  \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
