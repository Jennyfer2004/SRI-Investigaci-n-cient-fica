{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5989783a",
   "metadata": {},
   "source": [
    "Consultas utilizadas para la API de extracción de artículos científicos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da44fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    \"machine learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"climate change\",\n",
    "    \"renewable energy\",\n",
    "    \"genomics\",\n",
    "    \"CRISPR\",\n",
    "    \"quantum computing\",\n",
    "    \"robotics\",\n",
    "    \"internet of things\",\n",
    "    \"blockchain\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision\",\n",
    "    \"cybersecurity\",\n",
    "    \"human-computer interaction\",\n",
    "    \"biomedical engineering\",\n",
    "    \"neuroscience\",\n",
    "    \"materials science\",\n",
    "    \"nanotechnology\",\n",
    "    \"agriculture technology\",\n",
    "    \"autonomous vehicles\",\n",
    "    \"edge computing\",\n",
    "    \"smart cities\",\n",
    "    \"5G networks\",\n",
    "    \"digital twins\",\n",
    "    \"data privacy\",\n",
    "    \"carbon capture\",\n",
    "    \"fusion energy\",\n",
    "    \"e-waste management\",\n",
    "    \"sustainable development\",\n",
    "    \"bioinformatics\",\n",
    "    \"ecosystem restoration\",\n",
    "    \"renewable hydrogen\",\n",
    "    \"space exploration\",\n",
    "    \"satellite data analysis\",\n",
    "    \"mental health technologies\",\n",
    "    \"disease modeling\",\n",
    "    \"covid-19 vaccine\",\n",
    "    \"epigenetics\",\n",
    "    \"artificial general intelligence\",\n",
    "    \"solar energy forecasting\",\n",
    "    \"digital pathology\",\n",
    "    \"quantum cryptography\",\n",
    "    \"explainable AI\",\n",
    "    \"precision agriculture\",\n",
    "    \"computational fluid dynamics\",\n",
    "    \"digital phenotyping\",\n",
    "    \"neuroprosthetics\",\n",
    "    \"climate resilient crops\",\n",
    "    \"bioinspired robotics\",\n",
    "    \"computational neuroscience\",\n",
    "    \"digital therapeutics\",\n",
    "    \"computational chemistry\",\n",
    "    \"synthetic biology\",\n",
    "    \"ocean acidification\",\n",
    "    \"wearable biosensors\",\n",
    "    \"environmental DNA\",\n",
    "    \"federated learning\",\n",
    "    \"molecular diagnostics\",\n",
    "    \"human microbiome\",\n",
    "    \"plastic degradation\",\n",
    "    \"cyber-physical systems\",\n",
    "    \"ecoacoustics\",\n",
    "    \"quantum sensors\",\n",
    "    \"computational linguistics\",\n",
    "    \"geomatics\",\n",
    "    \"urban heat islands\",\n",
    "    \"space weather modeling\",\n",
    "    \"digital biomarkers\",\n",
    "    \"smart manufacturing\",\n",
    "    \"metaverse technologies\",\n",
    "    \"livestock genomics\",\n",
    "    \"AI in healthcare\",\n",
    "    \"bioelectronic medicine\",\n",
    "    \"AI for education\",\n",
    "    \"digital agriculture\",\n",
    "    \"energy harvesting materials\",\n",
    "    \"self-healing materials\",\n",
    "    \"AI-driven drug discovery\",\n",
    "    \"extreme weather prediction\",\n",
    "    \"quantum machine learning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb5b2c",
   "metadata": {},
   "source": [
    "Utilizamos la API de crossref para extraer información de artículos científicos y sus dois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31eabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuración desde .env\n",
    "EMAIL = os.getenv(\"email\")\n",
    "MAX_PER_QUERY = 250\n",
    "TOTAL_MAX = 2000\n",
    "YEARS_BACK = 5\n",
    "\n",
    "# Archivos de control\n",
    "VISITED_DOIS_FILE = \"dois_visitados.txt\"\n",
    "OUTPUT_FILE = \"papers_guardados.csv\"\n",
    "\n",
    "# Cargar DOIs ya procesados\n",
    "existing_dois = set()\n",
    "if os.path.exists(VISITED_DOIS_FILE):\n",
    "    with open(VISITED_DOIS_FILE, \"r\") as f:\n",
    "        existing_dois.update(line.strip() for line in f)\n",
    "\n",
    "results = []\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existente = pd.read_csv(OUTPUT_FILE)\n",
    "    results = df_existente.to_dict(orient=\"records\")\n",
    "\n",
    "total_collected = len(results)\n",
    "\n",
    "def fetch_crossref_papers(query: str, max_results: int = 100, years_back: int = 5, offset: int = 0) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Busca artículos científicos en Crossref con DOI y filtro por fecha.\n",
    "    \"\"\"\n",
    "    url = \"https://api.crossref.org/works\" \n",
    "    fecha_limite = (datetime.now() - timedelta(days=years_back * 365)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'rows': max_results,\n",
    "        'offset': offset,\n",
    "        'filter': f\"from-pub-date:{fecha_limite}\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        papers = []\n",
    "        for item in data.get(\"message\", {}).get(\"items\", []):\n",
    "            if item.get(\"language\", \"en\") != \"en\":\n",
    "                continue\n",
    "\n",
    "            title = item.get(\"title\", [\"\"])[0]\n",
    "            doi = item.get(\"DOI\")\n",
    "            if not doi or doi in existing_dois:\n",
    "                continue\n",
    "\n",
    "            landing_page = f\"https://doi.org/{doi}\" \n",
    "            authors = item.get(\"author\", [])\n",
    "            authors_str = \", \".join([f\"{a.get('given', '')} {a.get('family', '')}\" for a in authors])\n",
    "\n",
    "            published_parts = item.get(\"published-print\", item.get(\"published-online\", {})).get(\"date-parts\", [])\n",
    "            published = \"-\".join(map(str, published_parts[0])) if published_parts else \"\"\n",
    "\n",
    "            abstract = item.get(\"abstract\", \"\")\n",
    "            if abstract:\n",
    "                abstract = BeautifulSoup(abstract, \"html.parser\").get_text()\n",
    "\n",
    "            papers.append({\n",
    "                \"titulo\": title,\n",
    "                \"autores\": authors_str,\n",
    "                \"publicado\": published,\n",
    "                \"idioma\": item.get(\"language\", \"en\"),\n",
    "                \"doi\": doi,\n",
    "                \"url\": landing_page,\n",
    "                \"abstract\": abstract,\n",
    "            })\n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en Crossref: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_open_access_pdf(doi: str, email: str = EMAIL) -> Optional[str]:\n",
    "    \"\"\"Busca un PDF abierto usando Unpaywall.\"\"\"\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\" \n",
    "    try:\n",
    "        response = requests.get(url, params={\"email\": email}).json()\n",
    "        if response.get(\"is_oa\"):\n",
    "            return response.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en Unpaywall: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_url: str) -> Optional[str]:\n",
    "    \"\"\"Extrae texto de un PDF dado su URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if \"application/pdf\" not in response.headers.get(\"Content-Type\", \"\"):\n",
    "            return None\n",
    "        with BytesIO(response.content) as f:\n",
    "            reader = PdfReader(f)\n",
    "            return \"\\n\".join(p.extract_text() or \"\" for p in reader.pages).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error leyendo PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_direct_pdf_link(url: str) -> Optional[str]:\n",
    "    \"\"\"Busca enlaces a PDFs dentro de la página.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for link in soup.find_all(\"a\", href=True):\n",
    "            if \".pdf\" in link[\"href\"].lower():\n",
    "                return requests.compat.urljoin(url, link[\"href\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error buscando PDF manualmente: {e}\")\n",
    "    return None\n",
    "\n",
    "# Consultas principales\n",
    "QUERIES = [\"machine learning\", \"climate change\", \"quantum computing\"]\n",
    "\n",
    "for query in QUERIES:\n",
    "    print(f\"\\n🔍 Consultando: {query}\")\n",
    "    for offset in range(0, MAX_PER_QUERY, 100):\n",
    "        if total_collected >= TOTAL_MAX:\n",
    "            break\n",
    "\n",
    "        papers = fetch_crossref_papers(query=query, max_results=100, years_back=YEARS_BACK, offset=offset)\n",
    "\n",
    "        with open(VISITED_DOIS_FILE, \"a\") as f_dois:\n",
    "            for paper in papers:\n",
    "                if total_collected >= TOTAL_MAX:\n",
    "                    break\n",
    "\n",
    "                doi = paper[\"doi\"]\n",
    "                if doi in existing_dois:\n",
    "                    continue\n",
    "\n",
    "                existing_dois.add(doi)\n",
    "                landing_url = paper[\"url\"]\n",
    "                pdf_url = get_open_access_pdf(doi)\n",
    "\n",
    "                if not pdf_url:\n",
    "                    pdf_url = find_direct_pdf_link(landing_url)\n",
    "\n",
    "                content = extract_text_from_pdf(pdf_url) if pdf_url else None\n",
    "                paper[\"contenido\"] = content\n",
    "\n",
    "                if content:\n",
    "                    results.append(paper)\n",
    "                    f_dois.write(doi + \"\\n\")\n",
    "                    total_collected += 1\n",
    "                    print(f\"✅ ({total_collected}) {paper['titulo'][:60]}...\")\n",
    "                    pd.DataFrame([paper]).to_csv(\n",
    "                        OUTPUT_FILE,\n",
    "                        mode=\"a\",\n",
    "                        header=not os.path.exists(OUTPUT_FILE),\n",
    "                        index=False,\n",
    "                        encoding='utf-8',\n",
    "                        errors='ignore'\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"⚠️ Sin contenido: {paper['titulo'][:60]}\")\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "print(f\"\\n✅ Total de papers útiles recopilados: {total_collected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea95b2b",
   "metadata": {},
   "source": [
    "Comprobamos que no existan aarticulos extraidos sin contenido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "OUTPUT_FILE = \"papers_guardados.csv\"\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existente = pd.read_csv(OUTPUT_FILE)\n",
    "\n",
    "    df_limpio = df_existente[df_existente['contenido'].notna() & (df_existente['contenido'] != '')]\n",
    "    df_limpio.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"Archivo limpiado guardado.: {len(df_limpio)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Documentos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
