{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5989783a",
   "metadata": {},
   "source": [
    "Consultas utilizadas para la API de extracción de artículos científicos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da44fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = [\n",
    "    \"machine learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"climate change\",\n",
    "    \"renewable energy\",\n",
    "    \"genomics\",\n",
    "    \"CRISPR\",\n",
    "    \"quantum computing\",\n",
    "    \"robotics\",\n",
    "    \"internet of things\",\n",
    "    \"blockchain\",\n",
    "    \"natural language processing\",\n",
    "    \"computer vision\",\n",
    "    \"cybersecurity\",\n",
    "    \"human-computer interaction\",\n",
    "    \"biomedical engineering\",\n",
    "    \"neuroscience\",\n",
    "    \"materials science\",\n",
    "    \"nanotechnology\",\n",
    "    \"agriculture technology\",\n",
    "    \"autonomous vehicles\",\n",
    "    \"edge computing\",\n",
    "    \"smart cities\",\n",
    "    \"5G networks\",\n",
    "    \"digital twins\",\n",
    "    \"data privacy\",\n",
    "    \"carbon capture\",\n",
    "    \"fusion energy\",\n",
    "    \"e-waste management\",\n",
    "    \"sustainable development\",\n",
    "    \"bioinformatics\",\n",
    "    \"ecosystem restoration\",\n",
    "    \"renewable hydrogen\",\n",
    "    \"space exploration\",\n",
    "    \"satellite data analysis\",\n",
    "    \"mental health technologies\",\n",
    "    \"disease modeling\",\n",
    "    \"covid-19 vaccine\",\n",
    "    \"epigenetics\",\n",
    "    \"artificial general intelligence\",\n",
    "    \"solar energy forecasting\",\n",
    "    \"digital pathology\",\n",
    "    \"quantum cryptography\",\n",
    "    \"explainable AI\",\n",
    "    \"precision agriculture\",\n",
    "    \"computational fluid dynamics\",\n",
    "    \"digital phenotyping\",\n",
    "    \"neuroprosthetics\",\n",
    "    \"climate resilient crops\",\n",
    "    \"bioinspired robotics\",\n",
    "    \"computational neuroscience\",\n",
    "    \"digital therapeutics\",\n",
    "    \"computational chemistry\",\n",
    "    \"synthetic biology\",\n",
    "    \"ocean acidification\",\n",
    "    \"wearable biosensors\",\n",
    "    \"environmental DNA\",\n",
    "    \"federated learning\",\n",
    "    \"molecular diagnostics\",\n",
    "    \"human microbiome\",\n",
    "    \"plastic degradation\",\n",
    "    \"cyber-physical systems\",\n",
    "    \"ecoacoustics\",\n",
    "    \"quantum sensors\",\n",
    "    \"computational linguistics\",\n",
    "    \"geomatics\",\n",
    "    \"urban heat islands\",\n",
    "    \"space weather modeling\",\n",
    "    \"digital biomarkers\",\n",
    "    \"smart manufacturing\",\n",
    "    \"metaverse technologies\",\n",
    "    \"livestock genomics\",\n",
    "    \"AI in healthcare\",\n",
    "    \"bioelectronic medicine\",\n",
    "    \"AI for education\",\n",
    "    \"digital agriculture\",\n",
    "    \"energy harvesting materials\",\n",
    "    \"self-healing materials\",\n",
    "    \"AI-driven drug discovery\",\n",
    "    \"extreme weather prediction\",\n",
    "    \"quantum machine learning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eb5b2c",
   "metadata": {},
   "source": [
    "Utilizamos la API de crossref para extraer información de artículos científicos y sus dois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31eabd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "EMAIL=os.getenv(\"email\")\n",
    "\n",
    "MAX_PER_QUERY = 250 \n",
    "TOTAL_MAX = 2000\n",
    "YEARS_BACK = 5\n",
    "\n",
    "VISITED_DOIS_FILE = \"dois_visitados.txt\"\n",
    "OUTPUT_FILE = \"papers_guardados.csv\"\n",
    "\n",
    "#Leyendo los archivos de documentos .csv por si existe no repetir dichas url\n",
    "if os.path.exists(VISITED_DOIS_FILE):\n",
    "    with open(VISITED_DOIS_FILE, \"r\") as f:\n",
    "        existing_dois = set(f.read().splitlines())\n",
    "else:\n",
    "    existing_dois = set()\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existente = pd.read_csv(OUTPUT_FILE)\n",
    "    dois_guardados = set(df_existente[\"doi\"])\n",
    "    results = df_existente.to_dict(orient=\"records\")\n",
    "else:\n",
    "    df_existente = pd.DataFrame()\n",
    "    dois_guardados = set()\n",
    "    results = []\n",
    "\n",
    "total_collected = len(results)\n",
    "\n",
    "\n",
    "def fetch_crossref_papers(query=\"machine learning\", max_results=5, years_back=5, offset=0,dois=None):\n",
    "    \"\"\" \n",
    "    Busca los artículos científicos que coincidan con la consulta y que no sean de más de cinco años\n",
    "    Args: \n",
    "       query(str)\n",
    "       max_results(int) : cantidad máxima de artículos buscados por consulta\n",
    "       years_back(int) : cantidad de años hacia atrás por los que filtramos los artículos\n",
    "       offset (int): Número de resultados a saltar (para paginación)\n",
    "       dois(list): Conjunto de identificadores DOI ya visitados\n",
    "    Returns:\n",
    "       list de diccionarios con la información de los artículos visitados \n",
    "    \"\"\"\n",
    "    url = \"https://api.crossref.org/works\"\n",
    "\n",
    "    fecha_limite = (datetime.now() - timedelta(days=years_back * 365)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'rows': max_results,\n",
    "        'offset': offset,\n",
    "        'filter': f\"from-pub-date:{fecha_limite}\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        papers = []\n",
    "        for item in data.get(\"message\", {}).get(\"items\", []):\n",
    "            if item.get(\"language\", \"en\") != \"en\":\n",
    "                continue\n",
    "\n",
    "            title = item.get(\"title\", [\"\"])[0]\n",
    "            doi = item.get(\"DOI\")\n",
    "            if doi in existing_dois:\n",
    "               continue\n",
    "            existing_dois.add(doi)\n",
    "\n",
    "            landing_page = f\"https://doi.org/{doi}\"\n",
    "\n",
    "            authors = item.get(\"author\", [])\n",
    "            authors_str = \", \".join([f\"{a.get('given', '')} {a.get('family', '')}\" for a in authors])\n",
    "\n",
    "            published_parts = item.get(\"published-print\", item.get(\"published-online\", {})).get(\"date-parts\", [])\n",
    "            published = \"-\".join(map(str, published_parts[0])) if published_parts else \"\"\n",
    "\n",
    "            abstract = item.get(\"abstract\", \"\")\n",
    "            if abstract:\n",
    "                abstract = BeautifulSoup(abstract, \"html.parser\").get_text()\n",
    "\n",
    "            papers.append({\n",
    "                \"title\": title,\n",
    "                \"doi\": doi,\n",
    "                \"landing_page\": landing_page,\n",
    "                \"authors\": authors_str,\n",
    "                \"published\": published,\n",
    "                \"abstract\": abstract,\n",
    "                \"language\": item.get(\"language\", \"en\")\n",
    "            })\n",
    "        return papers\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en Crossref: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_open_access_pdf(doi, email=EMAIL):\n",
    "    \"\"\"\n",
    "    Busca con la api unpaywall el contenidos de los artículos\n",
    "    Args:\n",
    "       doi(str): identificador del artículo\n",
    "       email(str): correo electrónico registrado para usar la API de Unpaywall.\n",
    "    Returns:\n",
    "       str: url del pdf\n",
    "    \"\"\"\n",
    "    url = f\"https://api.unpaywall.org/v2/{doi}\"\n",
    "    params = {\"email\": email}\n",
    "    try:\n",
    "        response = requests.get(url, params=params).json()\n",
    "        if response.get(\"is_oa\"):\n",
    "            return response.get(\"best_oa_location\", {}).get(\"url_for_pdf\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en Unpaywall: {e}\")\n",
    "    return None\n",
    "\n",
    "def find_direct_pdf_link(url):\n",
    "    \"\"\"\n",
    "    Intenta encontrar un enlace directo a PDF en una página web\n",
    "    Args:\n",
    "        url(str): url de la página donde buscar el enlace del pdf\n",
    "    Returns:\n",
    "        str: url completa del PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if \"text/html\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                if \".pdf\" in link[\"href\"].lower():\n",
    "                    return requests.compat.urljoin(url, link[\"href\"])\n",
    "    except Exception as e:\n",
    "        print(f\"❌ PDF manual falló: {e}\")\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_url):\n",
    "    \"\"\"\n",
    "    Extrae el texto de un archivo PDF desde una url\n",
    "    Args:\n",
    "        pdf_url(str): url del pdf\n",
    "    Returns:\n",
    "        str: Texto extraído del PDF\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        if \"application/pdf\" not in response.headers.get(\"Content-Type\", \"\"):\n",
    "            return None\n",
    "        with BytesIO(response.content) as f:\n",
    "            reader = PdfReader(f)\n",
    "            return \"\\n\".join(filter(None, (p.extract_text() for p in reader.pages))).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error leyendo PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "#iteración por las consultas\n",
    "for query in QUERIES:\n",
    "    print(f\"\\n🔍 Consultando: {query}\")\n",
    "    for offset in range(0, MAX_PER_QUERY, 100):\n",
    "        if total_collected >= TOTAL_MAX:\n",
    "            break\n",
    "\n",
    "        papers = fetch_crossref_papers(query=query, max_results=100, years_back=YEARS_BACK, offset=offset,dois=existing_dois)\n",
    "\n",
    "        with open(VISITED_DOIS_FILE, \"a\") as f_dois:\n",
    "            for paper in papers:\n",
    "                doi = paper[\"doi\"]\n",
    "                if doi in dois_guardados:\n",
    "                    continue\n",
    "                existing_dois.add(doi)\n",
    "                landing_url = paper[\"landing_page\"]\n",
    "                pdf_url = get_open_access_pdf(doi)\n",
    "                if not pdf_url:\n",
    "                    pdf_url = find_direct_pdf_link(landing_url)\n",
    "\n",
    "                paper[\"pdf_url\"] = pdf_url\n",
    "                content = extract_text_from_pdf(pdf_url) if pdf_url else None\n",
    "                paper[\"contenido\"] = content\n",
    "\n",
    "                if content:\n",
    "                    results.append(paper)\n",
    "                    dois_guardados.add(doi)\n",
    "                    f_dois.write(doi + \"\\n\")\n",
    "                    total_collected += 1\n",
    "\n",
    "                    print(f\"✅ ({total_collected}) {paper['title'][:60]}...\")\n",
    "                    \n",
    "                    pd.DataFrame([paper]).to_csv(OUTPUT_FILE, mode=\"a\", header=not os.path.exists(OUTPUT_FILE), index=False, encoding='utf-8',errors='ignore')\n",
    "                else:\n",
    "                    print(f\"⚠️ Sin contenido: {paper['title'][:60]}\")\n",
    "\n",
    "                if total_collected >= TOTAL_MAX:\n",
    "                    break\n",
    "\n",
    "                time.sleep(1)  # Evitar baneos\n",
    "\n",
    "print(f\"\\n✅ Total de papers útiles recopilados: {total_collected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea95b2b",
   "metadata": {},
   "source": [
    "Comprobamos que no existan aarticulos extraidos sin contenido "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "OUTPUT_FILE = \"papers_guardados.csv\"\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_existente = pd.read_csv(OUTPUT_FILE)\n",
    "\n",
    "    df_limpio = df_existente[df_existente['contenido'].notna() & (df_existente['contenido'] != '')]\n",
    "    df_limpio.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"Archivo limpiado guardado.: {len(df_limpio)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Documentos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
